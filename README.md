# üöÄ MLP from Scratch para MNIST

Este projeto √© uma implementa√ß√£o de uma rede neural Multi-Layer Perceptron (MLP) do zero, utilizando principalmente a biblioteca **NumPy** para reconhecer d√≠gitos manuscritos do conjunto de dados MNIST. O objetivo √© demonstrar o funcionamento interno de uma rede neural, incluindo feed forward e backpropagation, sem o uso de frameworks de alto n√≠vel como TensorFlow ou PyTorch.

![Python](https://img.shields.io/badge/Python-3776AB?style=for-the-badge&logo=python&logoColor=white)
![NumPy](https://img.shields.io/badge/NumPy-013243?style=for-the-badge&logo=numpy&logoColor=white)
![Matplotlib](https://img.shields.io/badge/Matplotlib-313131?style=for-the-badge&logo=matplotlib&logoColor=white)

## üéØ Sobre o Projeto

O foco principal √© construir e treinar uma rede neural simples "do zero" (from scratch) para a tarefa de classifica√ß√£o de imagens. O conjunto de dados MNIST, que cont√©m 70.000 imagens de d√≠gitos de 0 a 9, √© utilizado como base de treinamento e teste.

## üìä Resultados

Ap√≥s o treinamento de 500 √©pocas, o modelo alcan√ßa uma acur√°cia significativa (cerca de 85.5% nos dados de treino, como visto no notebook).

O notebook tamb√©m inclui uma c√©lula interativa que permite testar o modelo treinado (`trained_model.pkl`) em exemplos aleat√≥rios do conjunto de teste, mostrando a predi√ß√£o e o r√≥tulo real:

![Predi√ß√£o da rede neural mostrando acerto](img/image.png)

## ü§ñ Arquitetura e Treinamento

O sistema foi desenvolvido seguindo os seguintes passos:

* **1. Carga e Prepara√ß√£o dos Dados:**
    * O conjunto de dados MNIST √© carregado usando a biblioteca TensorFlow (apenas para o download do dataset).
    * Os valores dos pixels das imagens (28x28) s√£o "achatados" (flattened) em vetores de 784 entradas.
    * Os dados de entrada s√£o normalizados para o intervalo [0, 1], dividindo-os por 255.0, o que facilita a converg√™ncia do treinamento.

* **2. Arquitetura da Rede Neural:**
    * A rede possui uma camada de entrada com 784 neur√¥nios (um para cada pixel).
    * Uma **camada escondida** com **10 neur√¥nios**, utilizando a fun√ß√£o de ativa√ß√£o **ReLU** (Rectified Linear Unit).
    * Uma **camada de sa√≠da** com **10 neur√¥nios**, correspondendo aos d√≠gitos de 0 a 9, utilizando a fun√ß√£o de ativa√ß√£o **Softmax** para gerar uma distribui√ß√£o de probabilidade.

* **3. Processo de Treinamento (do Zero):**
    * **Feed Forward:** As entradas s√£o propagadas pela rede, multiplicadas pelos pesos e somadas aos *bias* de cada camada, at√© gerar a previs√£o na sa√≠da.
    * **Backpropagation:** O erro entre a previs√£o (sa√≠da do Softmax) e o r√≥tulo real (convertido para *one-hot encoding*) √© calculado. Esse erro √© propagado de volta pela rede para calcular os gradientes dos pesos e *bias*.
    * **Descida do Gradiente:** Os pesos e *bias* da rede s√£o ajustados em pequenas etapas (controladas pela taxa de aprendizado, `alpha`) na dire√ß√£o oposta do gradiente, minimizando o erro ao longo de 500 itera√ß√µes (√©pocas).

* **4. Serializa√ß√£o do Modelo:**
    * Ap√≥s o treinamento, os pesos (Wh, Ws) e bias (bh, bs) da rede s√£o salvos em um arquivo `trained_model.pkl` usando a biblioteca `pickle`. Isso permite que o modelo seja carregado e reutilizado sem necessidade de novo treinamento.

## üõ†Ô∏è Tecnologias Utilizadas

* **Python**
* **NumPy:** Utilizado para todas as opera√ß√µes de matrizes, implementa√ß√£o do feed-forward, backpropagation e fun√ß√µes de ativa√ß√£o.
* **Matplotlib:** Para visualiza√ß√£o das imagens dos d√≠gitos.
* **TensorFlow (Keras):** Utilizado exclusivamente para facilitar o download e carregamento do *dataset* MNIST.
* **IPyWidgets:** Para criar o bot√£o interativo de demonstra√ß√£o no Jupyter.
* **Pickle:** Para salvar e carregar o modelo treinado.
